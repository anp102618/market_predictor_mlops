{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f521663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\prasa\\anaconda3\\envs\\sp-env\\lib\\site-packages (from beautifulsoup4->bs4) (4.13.2)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ---------------------------------------- 3/3 [bs4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.4 bs4-0.0.2 soupsieve-2.7\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6237cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published Date: None\n",
      "Article Text:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://in.investing.com/news/stock-market-news/nifty-rangebound-ahead-of-rbi-policy-sebi-ras-expect-subdued-expiry-session-4863820\"\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Referer\": \"https://www.google.com/\"\n",
    "}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract date\n",
    "# The date is inside a span with class \"articleTime\"\n",
    "date_span = soup.find(\"span\", class_=\"articleTime\")\n",
    "if date_span:\n",
    "    date_str = date_span.text.strip()\n",
    "    # Example date_str: \"Jun 4, 2025 3:22PM IST\"\n",
    "    # Parse date string\n",
    "    try:\n",
    "        published_date = datetime.strptime(date_str, \"%b %d, %Y %I:%M%p IST\")\n",
    "    except ValueError:\n",
    "        published_date = date_str  # fallback to raw text if parsing fails\n",
    "else:\n",
    "    published_date = None\n",
    "\n",
    "# Extract paragraphs inside the article container\n",
    "article_div = soup.find(\"div\", id=\"article\")\n",
    "paragraphs = article_div.find_all(\"p\") if article_div else []\n",
    "\n",
    "full_text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "print(\"Published Date:\", published_date)\n",
    "print(\"Article Text:\\n\", full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_performance\n",
    "    def get_closing_prices1(self, ticker):\n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=self.chrome_options)\n",
    "            url = f'https://finance.yahoo.com/quote/'+ticker+'/history'\n",
    "            driver.get(url)\n",
    "            table = driver.find_element(By.XPATH, \"//div[@class='container' and @data-testid='history-table']//table\")\n",
    "            rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "                if not cols:\n",
    "                    cols = row.find_elements(By.TAG_NAME, 'th')\n",
    "                data.append([col.text.strip() for col in cols])\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:]\n",
    "            df.rename(columns={'Date': 'date', 'Close': 'close'}, inplace=True)\n",
    "\n",
    "            df['date'] = pd.to_datetime(df['date'].apply(lambda x: datetime.strptime(x, \"%b %d, %Y\")))\n",
    "            df['close'] = df['close'].apply(lambda x: float(x.replace(\",\", \"\")))\n",
    "\n",
    "            driver.quit()\n",
    "            return df[['date', 'close']]\n",
    "        except CustomException as e:\n",
    "            logger.error(f\"[{ticker}] Failed to scrape data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    " @track_performance\n",
    "    def extract_text(self, url):\n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=self.chrome_options)\n",
    "            driver.get(url)\n",
    "\n",
    "            paragraphs = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located(\n",
    "                (By.XPATH, \"//*[@id='article'][@class='article_container']//p\")\n",
    "            ))\n",
    "            date_text = WebDriverWait(driver, 10).until(EC.visibility_of_element_located(\n",
    "                (By.XPATH, \"//span[contains(text(),'Published')]\")\n",
    "            )).text\n",
    "\n",
    "            extracted_date = datetime.strptime(date_text.split(\" \")[1].strip().replace(\",\", \"\"), \"%d-%m-%Y\").strftime(\"%Y-%m-%d\")\n",
    "            full_text = \" \".join([p.text for p in paragraphs if p.text.strip()])\n",
    "            return [extracted_date, full_text]\n",
    "        except CustomException as e:\n",
    "            logger.error(f\"[SCRAPE ERROR] URL: {url}, Error: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if 'driver' in locals():\n",
    "                driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775cf2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  mean_sentiment_score\n",
      "0  2020-09-03              0.456088\n",
      "1  2020-09-04              0.496634\n",
      "2  2020-09-07              0.579715\n",
      "3  2020-09-08              0.526186\n",
      "4  2020-09-09              0.186779\n",
      "            date  mean_sentiment_score\n",
      "1402  2025-05-30              0.701223\n",
      "1403  2025-06-02              0.624525\n",
      "1404  2025-06-03              0.608422\n",
      "1405  2025-06-04              0.573783\n",
      "1406  2025-06-05              0.433324\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Connect to your SQLite database\n",
    "conn = sqlite3.connect(\"Data/data.db\")  # Example: \"data/my_data.db\"\n",
    "\n",
    "# Step 2: Fetch the table into a DataFrame\n",
    "table_name = \"news_data\"  # Replace with your actual table name\n",
    "df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "\n",
    "# Step 3: (Optional) Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Step 4: View your DataFrame\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db611d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-07 23:12:21,728] INFO - Running 'apply'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-07 23:12:21,908] INFO - Successfully applied scaling and data splitting.\n",
      "[2025-06-07 23:12:21,918] INFO - 'apply' completed in 0.1902 sec\n",
      "[2025-06-07 23:12:21,921] INFO - Memory used: 640.91 KB (peak: 878.40 KB)\n"
     ]
    }
   ],
   "source": [
    "from Model_Utils.feature_splitting_scaling import ScalingWithSplitStrategy\n",
    "import pandas as pd\n",
    "splitter = ScalingWithSplitStrategy()\n",
    "df = pd.read_csv(\"Data/processed_data/preprocessed_data.csv\", index_col=[0])\n",
    "df1 = df.drop(columns=['date'])\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splitter.apply(df1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d54df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>mean_sentiment_score</th>\n",
       "      <th>nasdaq</th>\n",
       "      <th>sp500</th>\n",
       "      <th>dj30</th>\n",
       "      <th>crude_oil</th>\n",
       "      <th>gold</th>\n",
       "      <th>usd_inr</th>\n",
       "      <th>10yb</th>\n",
       "      <th>vix</th>\n",
       "      <th>...</th>\n",
       "      <th>sp500_pct_chg</th>\n",
       "      <th>dj30_pct_chg</th>\n",
       "      <th>crude_oil_pct_chg</th>\n",
       "      <th>gold_pct_chg</th>\n",
       "      <th>usd_inr_pct_chg</th>\n",
       "      <th>10yb_pct_chg</th>\n",
       "      <th>vix_pct_chg</th>\n",
       "      <th>nsebank_pct_chg</th>\n",
       "      <th>nsei_pct_chg</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>0.481459</td>\n",
       "      <td>19529.95</td>\n",
       "      <td>6000.36</td>\n",
       "      <td>42762.87</td>\n",
       "      <td>60.292708</td>\n",
       "      <td>0.23187</td>\n",
       "      <td>85.88</td>\n",
       "      <td>4.51</td>\n",
       "      <td>1.391643</td>\n",
       "      <td>...</td>\n",
       "      <td>1.028067</td>\n",
       "      <td>1.0471</td>\n",
       "      <td>2.163219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.023283</td>\n",
       "      <td>2.733485</td>\n",
       "      <td>-1.179332</td>\n",
       "      <td>1.466172</td>\n",
       "      <td>1.018751</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  mean_sentiment_score    nasdaq    sp500      dj30  \\\n",
       "1147  2025-06-06              0.481459  19529.95  6000.36  42762.87   \n",
       "\n",
       "      crude_oil     gold  usd_inr  10yb       vix  ...  sp500_pct_chg  \\\n",
       "1147  60.292708  0.23187    85.88  4.51  1.391643  ...       1.028067   \n",
       "\n",
       "      dj30_pct_chg  crude_oil_pct_chg  gold_pct_chg  usd_inr_pct_chg  \\\n",
       "1147        1.0471           2.163219           0.0        -0.023283   \n",
       "\n",
       "      10yb_pct_chg  vix_pct_chg  nsebank_pct_chg  nsei_pct_chg  target  \n",
       "1147      2.733485    -1.179332         1.466172      1.018751     NaN  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5efc7234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-07 12:12:49,471] INFO - Running 'apply'...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 22)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m features \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m last_row \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39miloc[[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m----> 3\u001b[0m last_row_scaled \u001b[38;5;241m=\u001b[39m \u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_row\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(last_row)\n",
      "File \u001b[1;32md:\\Stock_Prediction\\Common_Utils\\__init__.py:104\u001b[0m, in \u001b[0;36mtrack_performance.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    102\u001b[0m tracemalloc\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m--> 104\u001b[0m result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    106\u001b[0m current, peak \u001b[38;5;241m=\u001b[39m tracemalloc\u001b[38;5;241m.\u001b[39mget_traced_memory()\n\u001b[0;32m    107\u001b[0m tracemalloc\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32md:\\Stock_Prediction\\Model_Utils\\feature_splitting_scaling.py:71\u001b[0m, in \u001b[0;36mScalingWithSplitStrategy.apply\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     68\u001b[0m val_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     69\u001b[0m test_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[train_df\u001b[38;5;241m.\u001b[39mindex, numeric_cols] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform(train_df[numeric_cols])\n\u001b[0;32m     74\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[val_df\u001b[38;5;241m.\u001b[39mindex, numeric_cols] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform(val_df[numeric_cols])\n",
      "File \u001b[1;32mc:\\Users\\prasa\\anaconda3\\envs\\sp-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prasa\\anaconda3\\envs\\sp-env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\prasa\\anaconda3\\envs\\sp-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prasa\\anaconda3\\envs\\sp-env\\lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\prasa\\anaconda3\\envs\\sp-env\\lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1134\u001b[0m         )\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 22)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "features = df.drop(columns=['date'], errors='ignore')\n",
    "last_row = features.iloc[[-1]]\n",
    "last_row_scaled = splitter.apply(last_row)\n",
    "print(last_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629d298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
